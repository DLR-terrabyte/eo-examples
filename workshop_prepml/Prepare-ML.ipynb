{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare a Training Data Set for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this tutorial is to: Extract data from **multiple** STAC collections only at the **specified locations** as a **time series**.\n",
    "\n",
    "As an example use case we are going to...\n",
    "1. Get Station Data:\n",
    "    - Download point data time series with a measured variable (e.g. snow depth)\n",
    "    - Preprocess it to be used with EO Data\n",
    "2. Get EO Data:\n",
    "    - Query a STAC catalog\n",
    "    - Get acquisitions for the points in the relevant time frame (temporal, spatial)\n",
    "    - Get acquisitions from different collections\n",
    "3. Prepare EO Data:\n",
    "    - Load the found items into one data cube, by only loading the relevant geometry\n",
    "    - Homogenize the datacube to a common temporal and spatial resolution\n",
    "    - Add new information to the data cube: Calculate the NDSI (*ideally the collections should be radiometrically harmonized, e.g. [sen2like](https://github.com/senbox-org/sen2like), we are not doing this for this tutorial*)\n",
    "    - Evaluate the gain in time steps \n",
    "4. Combine EO and Station Data:\n",
    "    - Convert the data cube and the station measurements\n",
    "    - Use a format that can be easily used for machine learning\n",
    "5. Apply Machine Learning\n",
    "    - Regression model to predict snow depth (this is exemplatory, not scientifically valid)\n",
    "6. Weather Data\n",
    "    - Compare to snow depth from ERA5 Land \n",
    "\n",
    "This tutorial should serve as guidance on how to extract data from multiple sources from STAC catalogs and use them in further workflows.\n",
    "There are many more applications that could be covered. The next step could be to add more predictors and do a multivariate regression, taking into account more factors like elevation, aspect, temperature, etc.\n",
    "\n",
    "Things to consider\n",
    "- sparse xarray data cubes\n",
    "- xvec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sketch_ws_prepml.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook needs a custom environment on terrabyte to run. Run the `micromamba install ...` cell below. Then close the session. Start a new jupyter session from the [terrabyte portal](https://portal.terrabyte.lrz.de/) where you specify the name (*prepml* in this example) of the newly created environment in the *custom environment field* (you have to type it there the first time you use it, then it will be available from the dropdown list above).  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# !micromamba create -y -n prepml requests numpy pandas geopandas xarray xvec rioxarray shapely odc-stac odc-geo pystac-client dask graphviz folium branca tensorflow seaborn libgdal libgdal-jp2openjpeg zarr jupyter jupyter-server-proxy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import time\n",
    "import socket\n",
    "import io\n",
    "import zipfile\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xarray\n",
    "import xvec\n",
    "import rioxarray\n",
    "from shapely.geometry import box, shape\n",
    "from odc import stac as odc_stac\n",
    "from odc.geo import geobox\n",
    "from pystac_client import Client as pystacclient\n",
    "from pystac.extensions.raster import RasterExtension\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "import folium\n",
    "import folium.plugins as folium_plugins\n",
    "import branca.colormap as cm\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Station data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download station data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download station data time series. We are going to use monthly snow depth measurements in South Tyrol. They have been prepared, gapfilled and made available via the ClirSnow project."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "# set url\n",
    "url = 'https://zenodo.org/records/5109574/files/meta_all.csv?download=1'\n",
    "filename = url.split('/')[-1]\n",
    "filename = filename.split('?')[0]\n",
    "\n",
    "# Send a GET request to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the file locally\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded and saved successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd_meta = pd.read_csv(filename)\n",
    "sd_meta.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# set url\n",
    "url = 'https://zenodo.org/records/5109574/files/data_monthly_IT_BZ.zip?download=1'\n",
    "filename = url.split('/')[-1]\n",
    "filename = filename.split('?')[0]\n",
    "\n",
    "# Send a GET request to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Open the downloaded file as a zip file\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        # Extract all contents into the current directory\n",
    "        zip_ref.extractall()\n",
    "    print(\"File downloaded and extracted successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# read it into memory\n",
    "filename = filename.split('.')[0] + '.csv'\n",
    "sd_mnth = pd.read_csv(filename)\n",
    "sd_mnth.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prepare station data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the metadata to keep only stations form the province of South Tyrol having values between 2000 and 2019. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "start = 2000\n",
    "end = 2019\n",
    "sd_meta = sd_meta[\n",
    "    (sd_meta['Provider'] == 'IT_BZ') &\n",
    "    (sd_meta['HS_year_start'] <= start) &\n",
    "    (sd_meta['HS_year_end'] >= end)]\n",
    "sd_meta = sd_meta[['Name', 'Longitude', 'Latitude', 'Elevation']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd_mnth = sd_mnth[['Name', 'year', 'month', 'HSmean_gapfill']]\n",
    "sd_mnth = sd_mnth[sd_mnth['year'].between(start, end)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "And join the metadata (geographical information, ...)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd = pd.merge(sd_meta, sd_mnth, on='Name', how='inner')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This is what our station data looks like now. The metadata combined with the measurements."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn snow depth time series into a geodataframe (we also do it for the metadata for plotting the locations)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd = gpd.GeoDataFrame(data=sd,\n",
    "                      geometry=gpd.points_from_xy(sd.Longitude, sd.Latitude),\n",
    "                      crs=\"EPSG:4326\")\n",
    "sd_meta = gpd.GeoDataFrame(data=sd_meta,\n",
    "                           geometry=gpd.points_from_xy(sd_meta.Longitude, sd_meta.Latitude),\n",
    "                           crs=\"EPSG:4326\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a buffer around the points for extracting more than just one pixel."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd_meta = sd_meta.to_crs(3035)  # LAEA Europe\n",
    "sd_meta['geometry'] = sd_meta['geometry'].buffer(distance=200, cap_style='square')  # square\n",
    "sd_meta = sd_meta.to_crs(4326)  # Back to 4326"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution of the stations and the buffers on a map."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "colormap = cm.linear.viridis.scale(round(sd_meta['Elevation'].min(), -2),\n",
    "                                   round(sd_meta['Elevation'].max(), -2))\n",
    "colormap.caption = 'Elevation m'\n",
    "\n",
    "m = folium.Map(tiles=\"OpenStreetMap\", zoom_start=9)\n",
    "\n",
    "folium.GeoJson(\n",
    "    sd_meta,\n",
    "    name=\"Snow Depth Stations Buffer\",\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['Elevation']),\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.7,\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(fields=[\"Name\", \"Elevation\"]),\n",
    ").add_to(m)\n",
    "\n",
    "folium.GeoJson(\n",
    "    sd_meta.geometry.centroid,\n",
    "    name=\"Snow Depth Stations\",\n",
    ").add_to(m)\n",
    "\n",
    "colormap.add_to(m)\n",
    "m.fit_bounds(m.get_bounds())\n",
    "m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Create a bounding box columns for querying the STAC catalog."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd_meta = pd.concat([sd_meta, sd_meta.bounds], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get EO Data: The terrabyte STAC Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Discover the STAC Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all available collections"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "catalog_url = 'https://stac.terrabyte.lrz.de/public/api'\n",
    "catalog = pystacclient.open(catalog_url)\n",
    "collections = catalog.get_all_collections()\n",
    "for collection in collections:\n",
    "    print(\n",
    "        f\"{collection.id} | {collection.title} | \"\n",
    "        f\"{collection.extent.temporal.intervals[0][0].year} - \"\n",
    "        f\"{collection.extent.temporal.intervals[0][1].year}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Check the band names for S2 C1 L2A, the LS collections"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "catalog.get_collection('sentinel-2-l2a')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Query data from the STAC catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Query a single station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the parameters we want to use for both collections."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "max_cloud_cover = 25\n",
    "\n",
    "query = {\n",
    "    'eo:cloud_cover': {\n",
    "        \"gte\": 0,\n",
    "        \"lte\": max_cloud_cover\n",
    "    }\n",
    "}\n",
    "\n",
    "start = '2000-01-01T00:00:00Z'\n",
    "end = '2019-12-31T23:59:59Z'\n",
    "\n",
    "bands = ['swir16',\n",
    "         'green']  # fortunately the bands have the same names across multiple collections. That's not always the case."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Sentinel-2 for one specific station. The first station in the list."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "collection = ['sentinel-2-c1-l2a']\n",
    "bbox = [sd_meta.minx.iloc[0], sd_meta.miny.iloc[0],\n",
    "        sd_meta.maxx.iloc[0], sd_meta.maxy.iloc[0]]  # first station in list\n",
    "\n",
    "search = catalog.search(collections=collection,\n",
    "                        bbox=bbox,\n",
    "                        datetime=[start, end],\n",
    "                        query=query)\n",
    "items = list(search.items())  # TODO: Is this still the correct way to do it?\n",
    "\n",
    "print(f'Found {len(items)} Scenes')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the search results. Full tiles are returned with their according metadata."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "items[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Sentinel-2 and all Landsat collections for the specified parameters and the same specific station."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "collection = ['landsat-ot-c2-l2', 'landsat-etm-c2-l2', 'landsat-tm-c2-l2',\n",
    "              'sentinel-2-c1-l2a']  # searching for each collection separately is more performant for larger requests!\n",
    "\n",
    "search = catalog.search(collections=collection,\n",
    "                        bbox=bbox,\n",
    "                        datetime=[start, end],\n",
    "                        query=query)\n",
    "items = list(search.items())  # TODO: Is this still the correct way to do it?\n",
    "\n",
    "print(f'Found {len(items)} Scenes')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This search, explicitly addressing the collections is more effective!"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "collection = ['landsat-ot-c2-l2', 'landsat-etm-c2-l2',\n",
    "              'landsat-tm-c2-l2', 'sentinel-2-c1-l2a']\n",
    "\n",
    "for col in collection:\n",
    "    search = catalog.search(collections=col,\n",
    "                            bbox=bbox,\n",
    "                            datetime=[start, end],\n",
    "                            query=query)\n",
    "    items = list(search.items())  # TODO: Is this still the correct way to do it?\n",
    "    print(f'Collection: {col}. Found {len(items)} Scenes')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Query multiple stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to use to iterate over all stations. bbox is the variable object in this function, it is extracted from each row of the station list."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def query_stac(row, collection):\n",
    "    bbox = [row.minx, row.miny, row.maxx, row.maxy]\n",
    "    search = catalog.search(collections=collection,\n",
    "                            bbox=bbox,\n",
    "                            datetime=[start, end],\n",
    "                            query=query)\n",
    "    items = list(search.items())\n",
    "    print(f\"Name: {row['Name']}, Items: {len(items)}\")\n",
    "    return items"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is applied to the station metadata geodataframe, where we had stored the buffers around the stations. The result is a list with all found STAC items for each of the stations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "collection = ['sentinel-2-c1-l2a']\n",
    "items_list_s2 = sd_meta.apply(query_stac, args=collection, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.2.3 Excursion: Use geometry of interest directly in search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your point/vector data is spread out across the globe, with many items in between the geometries, it is better to use the geometry explicitly in the STAC search and not the bounding box. In this way you will only get the tiles you are interested in.\n",
    "\n",
    "If your point/vector data is close to one another, with few items in between the geometries, it is better to use the bounding box of your geometries in the STAC search. In this way you will not duplicate any items in your search. This is what we will do know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Query full bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the full bounding box of our station network to the search."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "collection = 'sentinel-2-c1-l2a'\n",
    "bbox = [sd_meta.minx.min(), sd_meta.miny.min(),\n",
    "        sd_meta.maxx.max(), sd_meta.maxy.max()]  # all stations\n",
    "\n",
    "search = catalog.search(collections=collection,\n",
    "                        bbox=bbox,\n",
    "                        datetime=[start, end],\n",
    "                        query=query)\n",
    "items_s2 = list(search.items())  # TODO: Is this still the correct way to do it?\n",
    "print(f'Collection: {collection}. Found {len(items_s2)} Scenes')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "collection = ['landsat-ot-c2-l2', 'landsat-etm-c2-l2', 'landsat-tm-c2-l2']\n",
    "\n",
    "items_ls = []\n",
    "for col in collection:\n",
    "    search = catalog.search(collections=col,\n",
    "                            bbox=bbox,\n",
    "                            datetime=[start, end],\n",
    "                            query=query)\n",
    "    items = list(search.items())  # TODO: Is this still the correct way to do it?\n",
    "    items_ls.append(items)\n",
    "    print(f'Collection: {col}. Found {len(items)} Scenes')\n",
    "\n",
    "# currently we have a list with 3 entries, let's flatten it\n",
    "items_ls = [item for sublist in items_ls for item in sublist]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's plot a some items in relation to a station. It becomes clear that loading all of this data should be avoided if possible!"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "map = folium.Map()\n",
    "layer_control = folium.LayerControl(position='topright', collapsed=True)\n",
    "\n",
    "tile_s2 = shape(items_s2[0].geometry)\n",
    "tile_s2 = gpd.GeoDataFrame([{'geometry': tile_s2}], crs=\"EPSG:4326\")\n",
    "tile_s2 = folium.GeoJson(tile_s2.to_json(), name=\"S2\",\n",
    "                         style_function=lambda x: {\"fillColor\": \"blue\"})\n",
    "\n",
    "tile_ls = shape(items_ls[0].geometry)\n",
    "tile_ls = gpd.GeoDataFrame([{'geometry': tile_ls}], crs=\"EPSG:4326\")\n",
    "tile_ls = folium.GeoJson(tile_ls.to_json(), name=\"LS\",\n",
    "                         style_function=lambda x: {\"fillColor\": \"green\"})\n",
    "\n",
    "aoi = box(*bbox)\n",
    "aoi = gpd.GeoDataFrame({\"geometry\": [aoi]}, crs=\"EPSG:4326\")\n",
    "aoi = folium.GeoJson(aoi.to_json(), name=\"aoi\",\n",
    "                     style_function=lambda x: {\"fillColor\": \"white\"})\n",
    "\n",
    "station = box(*[sd_meta.minx.iloc[0], sd_meta.miny.iloc[0],\n",
    "                sd_meta.maxx.iloc[0], sd_meta.maxy.iloc[0]])\n",
    "station = gpd.GeoDataFrame({\"geometry\": [station]}, crs=\"EPSG:4326\")\n",
    "station_mark = station.geometry.centroid\n",
    "station = folium.GeoJson(station.to_json(), name=\"Station\",\n",
    "                         style_function=lambda x: {\"fillColor\": \"red\"})\n",
    "\n",
    "station_mark = folium.GeoJson(station_mark, name=\"Station Marker\")\n",
    "\n",
    "tile_s2.add_to(map)\n",
    "tile_ls.add_to(map)\n",
    "station.add_to(map)\n",
    "aoi.add_to(map)\n",
    "station_mark.add_to(map)\n",
    "layer_control.add_to(map)\n",
    "map.fit_bounds(map.get_bounds())\n",
    "map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Starting a Dask Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are starting the dask client for scaling the computation to the available resources.\n",
    "Once started, a link to the dask dashboard will be shown which will display details on the dask computation status.\n",
    "This should be done **before** the first calculation on xarray objects takes place!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dir_out = '~/ws_prepml'\n",
    "dask_tmpdir = os.path.join(dir_out, 'scratch', 'localCluster')\n",
    "# from testins running without threads is the faster option \n",
    "dask_threads = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "host = os.getenv('host')\n",
    "jl_port = os.getenv('port')\n",
    "#create to URL to point to the jupyter-server-proxy\n",
    "dask_url = f'https://portal.terrabyte.lrz.de/node/{host}/{jl_port}' + '/proxy/{port}/status'\n",
    "#dask will insert the final port chosen by the Cluster\n",
    "\n",
    "dask.config.set({'temporary_directory': dask_tmpdir,\n",
    "                 'distributed.dashboard.link': dask_url})\n",
    "\n",
    "#some settings to increase network timeouts and allow the dashboard to plot larger graphs\n",
    "#dask.config.set({'distributed.comm.timeouts.tcp': '180s',\n",
    "#                 'distributed.comm.timeouts.connect': '120s',\n",
    "#                 'distributed.dashboard.graph-max-items': 55000,\n",
    "#                 'distributed.deploy.lost-worker-timeout': '90s',\n",
    "#                 'distributed.scheduler.allowed-failures': 180,\n",
    "#                 })\n",
    "\n",
    "#we set the dashboard address for dask to choose a free random port,\n",
    "# so there is no error with multiple tasks running on same node\n",
    "client = Client(threads_per_worker=dask_threads,\n",
    "                dashboard_address=\"127.0.0.1:0\")\n",
    "client"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the workflow on a small subset of the data**. Before the workflow is mature it needs to be tested. This should be done on a small subset of the data set to: reduce processing time, save resources, get a feeling for the value ranges etc. Let's develop the workflow on a single station before expanding to all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Reduce amount of data for quick prototyping\n",
    "First, some tweaks to reduce the amount of data.\n",
    "*Note: Do this for s2, ls, and then also combine the two into one cube.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def filter_time(items, start_date, end_date):\n",
    "    items_tst = [\n",
    "        item for item in items\n",
    "        if start_date <= item.datetime <= end_date\n",
    "    ]\n",
    "    return items_tst"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# choose the timerange for prototyping\n",
    "start_date = datetime(2017, 1, 1, tzinfo=timezone.utc)\n",
    "end_date = datetime(2018, 1, 1, tzinfo=timezone.utc)\n",
    "\n",
    "# define the parameters for prototyping per collection\n",
    "proto_dict = {\n",
    "    \"s2\": {\n",
    "        \"items_tst\": filter_time(items_s2, start_date, end_date),\n",
    "        \"scale\": RasterExtension.ext(items_s2[0].assets[\"B03\"]).bands[0].scale,\n",
    "        # 0.0001; it is constant across the relevant bands and time steps\n",
    "        \"offset\": RasterExtension.ext(items_s2[0].assets[\"B03\"]).bands[0].offset,\n",
    "        # -0.1; it is constant across the relevant bands and time steps\n",
    "    },\n",
    "    \"ls\": {\n",
    "        \"items_tst\": filter_time(items_ls, start_date, end_date),\n",
    "        \"scale\": RasterExtension.ext(items_ls[0].assets[\"B03\"]).bands[0].scale,\n",
    "        # 2.75e-05; it is constant across the relevant bands and time steps\n",
    "        \"offset\": RasterExtension.ext(items_ls[0].assets[\"B03\"]).bands[0].offset,\n",
    "        # -0.2; it is constant across the relevant bands and time steps\n",
    "    }\n",
    "}\n",
    "\n",
    "# aoi\n",
    "aoi_tst = sd_meta.iloc[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a reduced data cube (**lazily - nothing is loaded so far**)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_s2 = odc_stac.load(proto_dict[\"s2\"][\"items_tst\"],\n",
    "                        geopolygon=aoi_tst.geometry,\n",
    "                        groupby='solar_day',\n",
    "                        chunks={\"time\": -1},  # keep time in one chunk.\n",
    "                        bands=bands,\n",
    "                        nodata=0,\n",
    "                        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_s2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_ls = odc_stac.load(proto_dict[\"ls\"][\"items_tst\"],\n",
    "                        geopolygon=aoi_tst.geometry,\n",
    "                        groupby='solar_day',\n",
    "                        chunks={\"time\": -1},  # keep time in one chunk.\n",
    "                        bands=bands,\n",
    "                        nodata=0,\n",
    "                        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_ls"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test the processing steps\n",
    "Scale, aggregate to monthly values (maximum value composite), calculate NDSI. **Still lazy.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def scale_and_offset(cube, scale, offset):\n",
    "    cube = cube.where(cube['green'] != 0)\n",
    "    cube = cube.where(cube['swir16'] != 0)\n",
    "    cube = cube * scale + offset\n",
    "    return cube\n",
    "\n",
    "\n",
    "def aggregate_monthly_ndsi(cube):\n",
    "    cube = cube.resample(\n",
    "        time=\"1ME\").median()  # using median here, since the collections are not spectrally harmonized - to ensure we keep information from both collections. max would be better.\n",
    "    cube[\"ndsi\"] = (cube.green - cube.swir16) / (cube.green + cube.swir16)\n",
    "    #cube = cube[\"ndsi\"]\n",
    "    return cube\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scale and Offset**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_s2 = scale_and_offset(cube_s2, scale=proto_dict['s2']['scale'],\n",
    "                           offset=proto_dict['s2']['offset'])\n",
    "cube_ls = scale_and_offset(cube_ls, scale=proto_dict['ls']['scale'],\n",
    "                           offset=proto_dict['ls']['offset'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge Cubes**\n",
    "\n",
    "This has to happen before calculating the monthly NDSI to increase the observations per month."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_s2_rep = cube_s2.rio.reproject_match(cube_ls)\n",
    "cube_mg = xarray.concat([cube_ls, cube_s2_rep], dim=\"time\")\n",
    "cube_mg = cube_mg.sortby(\"time\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monthly NDSI**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_s2 = aggregate_monthly_ndsi(cube=cube_s2)\n",
    "cube_ls = aggregate_monthly_ndsi(cube=cube_ls)\n",
    "cube_mg = aggregate_monthly_ndsi(cube=cube_mg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect**\n",
    "\n",
    "Load a couple of time steps to check the values! Change the bands for more inspection. **Now data is loaded!**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "cube_s2['ndsi'].isel(time=slice(0, 5)).plot.imshow(col=\"time\", size=8, aspect=1, vmin=-1, vmax=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "cube_ls['ndsi'].isel(time=slice(0, 5)).plot.imshow(col=\"time\", size=8, aspect=1, vmin=-1, vmax=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "cube_mg['ndsi'].isel(time=slice(0, 5)).plot.imshow(col=\"time\", size=8, aspect=1, vmin=-1, vmax=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's look at the values as well!"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "cube_mg.isel(time=slice(0, 5)).load()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate Spatially**\n",
    "\n",
    "Aggregate the cube spatially to get a time series for the whole period we chose. This is to check if the seasonality makes sense."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "cube_s2['ndsi'].mean(dim=[\"x\", \"y\"]).plot(ylim=[-1, 1])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "cube_ls['ndsi'].mean(dim=[\"x\", \"y\"]).plot(ylim=[-1, 1])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "cube_mg['ndsi'].mean(dim=[\"x\", \"y\"]).plot(ylim=[-1, 1])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.3 Compare to station data\n",
    "Get the snow depth data for the test station"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "aoi_tst"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what the actual measurements look like."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "sd_mnth[(sd_mnth['Name'] == aoi_tst.Name)].head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a time column in date format that matches the date format in the data cube."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd_mnth['time'] = pd.to_datetime(\n",
    "    sd_mnth['year'].astype(str) + '-' + sd_mnth['month'].astype(str)) + pd.offsets.MonthEnd(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to the chosen time range and station."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd_tst = sd_mnth[(sd_mnth['Name'] == aoi_tst.Name) &\n",
    "                 (sd_mnth['time'] >= cube_mg.time.min().values) &\n",
    "                 (sd_mnth['time'] <= cube_mg.time.max().values)]\n",
    "sd_tst.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Convert the NDSI time series to a data frame."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "df_ts = cube_mg['ndsi'].mean(dim=[\"x\", \"y\"]).to_dataframe()\n",
    "df_ts.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the two time series by date."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd_tst = pd.merge(sd_tst, df_ts, on=\"time\")\n",
    "sd_tst"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot their relationship (as a time series and as a scatter plot)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = sd_tst.plot(x=\"time\", y=\"HSmean_gapfill\", label=\"HSmean_gapfill\",\n",
    "                 marker=\"o\", color=\"blue\", figsize=(10, 6))\n",
    "ax.set_ylabel(\"HSmean_gapfill\", color=\"blue\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "sd_tst.plot(x=\"time\", y=\"ndsi\", label=\"ndsi\", marker=\"s\", color=\"orange\", ax=ax2)\n",
    "ax2.set_ylabel(\"NDSI\", color=\"orange\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "sd_tst.plot.scatter(x=\"ndsi\", y=\"HSmean_gapfill\", figsize=(10, 6))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Delete unneeded objects."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "del sd_tst\n",
    "del df_ts\n",
    "del cube_mg\n",
    "del cube_s2_rep\n",
    "del cube_ls\n",
    "del cube_s2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling up to the full extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Defining the data cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Converting the bounding box of the station network we used above to a geodataframe"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "aoi = box(*bbox)\n",
    "aoi = gpd.GeoDataFrame({\"geometry\": [aoi]}, crs=\"EPSG:4326\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Sentinel-2 data cube"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "bands = [\"green\", \"swir16\"]  # s2 and ls share the same band names\n",
    "chunk_size = 512\n",
    "\n",
    "cube_s2 = odc_stac.load(items_s2,\n",
    "                        geopolygon=aoi.geometry,\n",
    "                        groupby='solar_day',\n",
    "                        chunks={\"x\": chunk_size, \"y\": chunk_size, \"time\": -1},\n",
    "                        bands=bands,\n",
    "                        resolution=60,  # going to 60 m resolution\n",
    "                        )\n",
    "cube_s2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Landsat data cube."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "chunk_size = 512\n",
    "cube_ls = odc_stac.load(items_ls,\n",
    "                        geopolygon=aoi.geometry,\n",
    "                        groupby='solar_day',\n",
    "                        chunks={\"x\": chunk_size, \"y\": chunk_size, \"time\": -1},\n",
    "                        bands=bands,\n",
    "                        resolution=60,\n",
    "                        )\n",
    "cube_ls"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Scale and Offset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_s2 = scale_and_offset(cube_s2, scale=proto_dict['s2']['scale'],\n",
    "                           offset=proto_dict['s2']['offset'])\n",
    "cube_ls = scale_and_offset(cube_ls, scale=proto_dict['ls']['scale'],\n",
    "                           offset=proto_dict['ls']['offset'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Merging the Cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Check the pixel sizes and CRS of the cubes."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def pixel_size_crs(cube):\n",
    "    x_p = np.diff(cube.x).mean()\n",
    "    y_p = np.diff(cube.y).mean()\n",
    "    cube_crs = cube.rio.crs\n",
    "    print(x_p)\n",
    "    print(y_p)\n",
    "    print(cube_crs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(pixel_size_crs(cube=cube_s2))\n",
    "print(pixel_size_crs(cube=cube_ls))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, the pixels are not aligned. One has to be reprojected."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "# takes forever, dask execution starts!\n",
    "# cube_s2_rep = cube_s2.rio.reproject_match(cube_ls)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge. And sort the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_mg = xarray.concat([cube_ls, cube_s2], dim=\"time\")\n",
    "cube_mg = cube_mg.sortby(\"time\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling the grid of the landsat data cube to twice it's size, aggregating using max().\n",
    "**This is a proxy for using the geometries to extract zonal statistics. It's more efficient.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#cube_ls = cube_ls.coarsen(x=2, y=2, boundary=\"trim\").max() # chunksize automatically halved."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#chunk_size = 1024\n",
    "#cube_ls = cube_ls.chunk({\"x\": chunk_size, \"y\": chunk_size})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling the grid of the s2 data cube to six times it's size (60 m), aggregating using max(). Then aligning to the landsat grid."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#cube_s2 = cube_s2.coarsen(x = 6, y = 6, boundary=\"trim\").max() # interp(x=cube_ls[\"x\"], y=cube_ls[\"y\"])   # https://github.com/pydata/xarray/issues/6799 --> doesn't work on chunks!"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#chunk_size = 1024\n",
    "#cube_s2 = cube_s2.chunk({\"x\": chunk_size, \"y\": chunk_size})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 NDSI and temporal aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal resampling to one month and calculating ndsi. (**lazy**)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_s2 = aggregate_monthly_ndsi(cube=cube_s2)\n",
    "cube_ls = aggregate_monthly_ndsi(cube=cube_ls)\n",
    "cube_mg = aggregate_monthly_ndsi(cube=cube_mg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cube_ls"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.4 Extract the NDSI at the stations"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the stations via xvec. In this workshop we are extracting the point locations. This is very efficient. That's why we have coarsened the resolution of our data cubes before. To simulate a spatial aggregation into zones. Using xvec zonal_statistics is possible but more costly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sd_meta.geometry.centroid.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_s2_points = cube_s2.xvec.extract_points(sd_meta.geometry.centroid, x_coords=\"x\",\n",
    "                                             y_coords=\"y\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_s2_points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "ndsi_s2_points = ndsi_s2_points[\"ndsi\"].compute()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ndsi_s2_ts = ndsi_s2_points.to_dataframe(dim_order=[\"geometry\", \"time\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_s2_ts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_s2_ts = ndsi_s2_ts.reset_index()\n",
    "ndsi_s2_ts = pd.merge(ndsi_s2_ts, sd_meta[[\"Name\", \"Elevation\"]], left_on=\"index\", right_index=True)\n",
    "ndsi_s2_ts = pd.merge(ndsi_s2_ts, sd_mnth, left_on=[\"Name\", \"time\"], right_on=[\"Name\", \"time\"])\n",
    "ndsi_s2_ts = ndsi_s2_ts[[\"time\", \"Name\", \"index\", \"Elevation\", \"ndsi\", \"HSmean_gapfill\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_s2_ts.hist(\"HSmean_gapfill\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Scatter plot\n",
    "df = ndsi_s2_ts[ndsi_s2_ts['ndsi'] > 0]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['ndsi'], df['HSmean_gapfill'], alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('NDSI')\n",
    "plt.ylabel('HSmean_gapfill')\n",
    "plt.title('Scatter Plot of NDSI vs HSmean_gapfill')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landsat"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_ls_points = cube_ls.xvec.extract_points(sd_meta.geometry.centroid,\n",
    "                                             x_coords=\"x\", y_coords=\"y\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_ls_points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Convert to data frames, and combine the NDSI time series."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "ndsi_ls_points = ndsi_ls_points.compute()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ndsi_ls_ts = ndsi_ls_points.to_dataframe(dim_order=[\"geometry\", \"time\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_ls_ts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Merge snow depth data. First metadata, then snow depth."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_ls_ts = ndsi_ls_ts.reset_index()\n",
    "ndsi_ls_ts = pd.merge(ndsi_ls_ts, sd_meta[[\"Name\", \"Elevation\"]], left_on=\"index\", right_index=True)\n",
    "ndsi_ls_ts = pd.merge(ndsi_ls_ts, sd_mnth, left_on=[\"Name\", \"time\"], right_on=[\"Name\", \"time\"])\n",
    "ndsi_ls_ts = ndsi_ls_ts[[\"time\", \"Name\", \"index\", \"Elevation\", \"ndsi\", \"HSmean_gapfill\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Scatter plot\n",
    "df = ndsi_ls_ts[ndsi_ls_ts['ndsi'] > 0]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['ndsi'], df['HSmean_gapfill'], alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('NDSI')\n",
    "plt.ylabel('HSmean_gapfill')\n",
    "plt.title('Scatter Plot of NDSI vs HSmean_gapfill')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged Cube\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_mg_points = cube_mg.xvec.extract_points(sd_meta.geometry.centroid,\n",
    "                                             x_coords=\"x\", y_coords=\"y\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_mg_points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "ndsi_mg_points = ndsi_mg_points.compute()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ndsi_mg_ts = ndsi_mg_points.to_dataframe(dim_order=[\"geometry\", \"time\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_mg_ts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndsi_mg_ts = ndsi_mg_ts.reset_index()\n",
    "ndsi_mg_ts = pd.merge(ndsi_mg_ts, sd_meta[[\"Name\", \"Elevation\"]], left_on=\"index\", right_index=True)\n",
    "ndsi_mg_ts = pd.merge(ndsi_mg_ts, sd_mnth, left_on=[\"Name\", \"time\"], right_on=[\"Name\", \"time\"])\n",
    "ndsi_mg_ts = ndsi_mg_ts[[\"time\", \"Name\", \"index\", \"Elevation\", \"ndsi\", \"HSmean_gapfill\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Scatter plot\n",
    "df = ndsi_mg_ts[ndsi_mg_ts['ndsi'] > 0]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['ndsi'], df['HSmean_gapfill'], alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('NDSI')\n",
    "plt.ylabel('HSmean_gapfill')\n",
    "plt.title('Scatter Plot of NDSI vs HSmean_gapfill')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.x Excursion: Extract via zonal_statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# replace nan with -2, then use normal rasterization\n",
    "# ndsi_s2 = ndsi_s2.fillna(-2)\n",
    "#print(sd_meta.crs)\n",
    "#sd_meta_rep = sd_meta.to_crs(ndsi_s2.rio.crs)\n",
    "#print(sd_meta_rep.crs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "##%%time\n",
    "#ndsi_s2_zonal = ndsi_s2.isel(time=slice(None, 5)).xvec.zonal_stats(\n",
    "#    list(sd_meta.geometry[0:5]), x_coords=\"x\", y_coords=\"y\", stats=\"max\", \n",
    "#    method=\"iterate\",\n",
    "#    n_jobs=-1,\n",
    "#    index=True,\n",
    "#)\n",
    "ndsi_s2_zonal\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "Next steps in this workflow could be:\n",
    "\n",
    "- Test an example where the stations are spread out over the globe.\n",
    "- Include cloud and quality maksing to get more robust results.\n",
    "- Use static information like elevation, aspect etc.\n",
    "- Apply more sophisticated machine learning:\n",
    "    - Time series prediction (extrapolation in time)\n",
    "    - Mapping (extrapolation in space)\n",
    "- Add more features (e.g. ERA5, S1) and do a mulitvariate analysis.\n",
    "- Find solution for xvec.zonal_stats(method=\"rasterize\")\n",
    "- try depth first approach (loop through workflow in prototyping)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
